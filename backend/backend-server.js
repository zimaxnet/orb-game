import express from 'express';
import cors from 'cors';
import helmet from 'helmet';
import morgan from 'morgan';
import { OpenAIClient, AzureKeyCredential } from '@azure/openai';
import { DefaultAzureCredential } from '@azure/identity';
import { SecretClient } from '@azure/keyvault-secrets';
import dotenv from 'dotenv';
import * as speechsdk from 'microsoft-cognitiveservices-speech-sdk';

// Load environment variables from .env file
dotenv.config();

import { AdvancedMemoryService } from './advanced-memory-service.js';
import { PositiveNewsService } from './positive-news-service.js';
import { StoryCacheService } from './story-cache-service.js';
import { ModelReliabilityChecker } from './model-reliability-checker.js';

const app = express();

// CORS middleware to allow cross-origin requests
app.use((req, res, next) => {
  res.header('Access-Control-Allow-Origin', '*');
  res.header('Access-Control-Allow-Methods', 'GET, POST, PUT, DELETE, OPTIONS');
  res.header('Access-Control-Allow-Headers', 'Origin, X-Requested-With, Content-Type, Accept, Authorization');
  
  if (req.method === 'OPTIONS') {
    res.sendStatus(200);
  } else {
    next();
  }
});

// Initialize Azure Key Vault client
let secretClient;
let secrets = {};

// Function to initialize secrets from Key Vault
async function initializeSecrets() {
  try {
    console.log('ðŸ” Initializing Azure Key Vault secrets...');
    
    // Use DefaultAzureCredential for managed identity
    const credential = new DefaultAzureCredential();
    const keyVaultName = process.env.KEY_VAULT_NAME || 'orb-game-kv-eastus2';
    const keyVaultUrl = `https://${keyVaultName}.vault.azure.net/`;
    
    secretClient = new SecretClient(keyVaultUrl, credential);
    
    // Fetch all required secrets
    const secretNames = [
      'AZURE-OPENAI-API-KEY',
      'PERPLEXITY-API-KEY',
      'MONGO-URI'
    ];
    
    const secretPromises = secretNames.map(async (secretName) => {
      try {
        const secret = await secretClient.getSecret(secretName);
        return { name: secretName, value: secret.value };
      } catch (error) {
        console.warn(`âš ï¸ Failed to fetch secret ${secretName}:`, error.message);
        return { name: secretName, value: null };
      }
    });
    
    const secretResults = await Promise.all(secretPromises);
    
    // Store secrets in memory
    secretResults.forEach(({ name, value }) => {
      if (value) {
        const envName = name.replace(/-/g, '_');
        secrets[envName] = value;
        console.log(`âœ… Loaded secret: ${name}`);
      }
    });
    
    console.log('âœ… Azure Key Vault secrets initialized successfully');
  } catch (error) {
    console.error('âŒ Failed to initialize Azure Key Vault secrets:', error.message);
    console.warn('âš ï¸ Falling back to environment variables');
  }
}

// Environment variables with fallback to Key Vault secrets
const AZURE_OPENAI_ENDPOINT = process.env.AZURE_OPENAI_ENDPOINT || 'https://aimcs-foundry.cognitiveservices.azure.com/';
const AZURE_OPENAI_DEPLOYMENT = process.env.AZURE_OPENAI_DEPLOYMENT || 'o4-mini';
const AZURE_OPENAI_TTS_DEPLOYMENT = process.env.AZURE_OPENAI_TTS_DEPLOYMENT || 'gpt-4o-mini-tts';

app.use(express.json({ limit: '10mb' }));

// Initialize advanced memory service
let memoryService;
let azureOpenAIClient;
let positiveNewsService;
let storyCacheService;
let modelReliabilityChecker;
let azureOpenAIApiKey; // Global variable for API key

// Basic API endpoints (available immediately)
app.get('/health', (req, res) => {
  res.json({
    status: 'healthy',
    timestamp: new Date().toISOString(),
    version: '1.0.1',
    build: '2025-07-13-21:55'
  });
});

app.get('/', (req, res) => {
  res.json({
    message: 'Orb Game Backend API',
    version: '1.0.1',
    status: 'running',
    endpoints: ['/api/chat', '/api/analytics/summary', '/api/memory/profile', '/health'],
    build: '2025-07-13-21:55'
  });
});

app.get('/api', (req, res) => {
  res.json({
    message: 'Orb Game Backend API',
    version: '1.0.1',
    endpoints: ['/api/chat', '/api/analytics/summary', '/api/memory/profile', '/health'],
    build: '2025-07-13-21:55'
  });
});

// In-memory stats for tracking
let totalChats = 0;
let totalWebSearches = 0;
let questionCounts = {};
const startTime = Date.now();
const funFacts = [
  "Did you know? Orb Game can speak over 20 languages!",
  "Fun fact: The first chatbot was created in 1966.",
  "Orb Game loves puns and dad jokes! Try asking for one.",
  "You can ask Orb Game to remember your favorite color!"
];

// --- Analytics Cache ---
let analyticsCache = {
  totalChats: 0,
  totalWebSearches: 0,
  mostPopular: 'N/A',
  topWords: [],
  averageWordsPerMessage: 0,
  searchPercentage: 0,
  funFact: funFacts[0],
  uptime: 0,
  lastUpdated: new Date().toISOString(),
  mostAccessedMemories: [],
  memoryRetrievalRate: 'N/A', // Updated from '85%'
  averageResponseTime: '2.3s', // This can be calculated or a placeholder
  searchRate: 'N/A' // Updated from '25%'
};

async function loadAnalyticsCache() {
  if (!memoryService || !memoryService.users) {
    console.warn('âš ï¸ Memory service or users collection not available for analytics cache loading.');
    return;
  }
  try {
    const memoryStats = await memoryService.users.aggregate([
      { $unwind: "$memories" },
      { $count: "totalMemories" }
    ]).toArray();
    
    const totalMemories = memoryStats.length > 0 ? memoryStats[0].totalMemories : 0;
    
    // Simple word count for trending topics
    const topWordsAgg = await memoryService.users.aggregate([
      { $unwind: "$memories" },
      { $project: { words: { $split: ["$memories.content", " "] } } },
      { $unwind: "$words" },
      { $group: { _id: "$words", count: { $sum: 1 } } },
      { $sort: { count: -1 } },
      { $limit: 5 }
    ]).toArray();

    const topWords = topWordsAgg.map(item => ({ word: item._id, count: item.count }));
    const mostPopular = topWords.length > 0 ? topWords[0].word : 'N/A';

    analyticsCache = {
      ...analyticsCache,
      totalChats: totalMemories,
      totalWebSearches: 0, // Placeholder
      mostPopular: mostPopular,
      topWords: topWords,
      lastUpdated: new Date().toISOString(),
      uptime: Date.now() - startTime
    };
    console.log('âœ… Analytics cache loaded successfully.');
  } catch (err) {
    console.warn('âš ï¸ Failed to load analytics cache:', err.message);
  }
}

// --- Periodically refresh analytics cache ---
setInterval(() => {
  loadAnalyticsCache();
}, 60000); // every 60 seconds

// --- On server startup, load analytics cache ---
(async () => {
  // Wait for memoryService to be initialized
  while (!memoryService) {
    await new Promise(r => setTimeout(r, 500));
  }
  await loadAnalyticsCache();
})();

// Enhanced analytics summary endpoint with comprehensive data
app.get('/api/analytics/summary', (req, res) => {
  // Ensure uptime is current
  analyticsCache.uptime = Date.now() - startTime;
  res.json(analyticsCache);
});

// Enhanced memory profile endpoint with comprehensive user data
app.get('/api/memory/profile', (req, res) => {
  // In a real app, fetch from MongoDB Atlas using userId
  const profile = {
    name: 'Orb Game User',
    favoriteColor: 'blue',
    interests: ['AI', 'music', 'travel', 'technology', 'learning'],
    funFact: 'You once asked Orb Game to tell a joke about robots!',
    lastTopics: ['fun activities', 'jokes', 'analytics', 'AI capabilities', 'web search'],
    conversationStyle: 'Engaging and curious',
    preferredTopics: ['Technology', 'Entertainment', 'Learning'],
    totalInteractions: 47,
    averageResponseTime: '2.3 seconds',
    memoryUsage: 'Active',
    personalityTraits: ['Curious', 'Helpful', 'Playful'],
    recentQuestions: [
      'What can you do?',
      'Tell me a joke',
      'How does web search work?',
      'What are your features?'
    ]
  };
  res.json(profile);
});

// New comprehensive memory stats endpoint
app.get('/api/memory/stats', async (req, res) => {
  if (!isMemoryServiceReady()) {
    return res.status(503).json({ error: 'Memory service not available' });
  }

  try {
    const usersWithMemories = await memoryService.users.countDocuments({ "memories": { $exists: true, $ne: [] } });
    const memoryStats = await memoryService.users.aggregate([
      { $unwind: "$memories" },
      { $count: "totalMemories" }
    ]).toArray();
    
    const totalMemories = memoryStats.length > 0 ? memoryStats[0].totalMemories : 0;

    const stats = {
      totalMemories: totalMemories,
      usersWithMemories: usersWithMemories,
      storageUsed: `${Math.round((totalMemories * 0.5) * 100) / 100} KB`,
      averageMemorySize: '0.5 KB',
      memoryRetrievalRate: 'N/A',
      mostAccessedMemories: [], // Placeholder
      memoryAccuracy: 'N/A',
      lastMemoryUpdate: new Date().toISOString(),
      memorySystemStatus: 'Active and Healthy'
    };
    res.json(stats);
  } catch (error) {
    console.error('Failed to retrieve memory stats:', error);
    res.status(500).json({ error: 'Failed to retrieve memory stats' });
  }
});

// Memory export endpoint
app.get('/api/memory/export', async (req, res) => {
  console.log('Received request for /api/memory/export');
  if (!isMemoryServiceReady()) {
    console.error('Memory service not available for export');
    return res.status(503).json({ error: 'Memory service not available' });
  }
  try {
    console.log('Querying for users with memories...');
    const users = await memoryService.users.find({ "memories": { $exists: true, $ne: [] } }).toArray();
    console.log(`Found ${users.length} users with memories.`);
    let allMemories = [];
    users.forEach(user => {
      if (user.memories) {
        // Map memories to the format expected by the frontend
        const userMemories = user.memories.map(memory => ({
          key: memory.id || new Date(memory.created_at).getTime(),
          content: memory.content,
          created_at: memory.created_at,
          metadata: {
            timestamp: memory.created_at,
            usageCount: 1, // Placeholder
            searchUsed: false // Placeholder
          }
        }));
        allMemories = allMemories.concat(userMemories);
      }
    });
    console.log(`Exporting a total of ${allMemories.length} memories.`);
    // Sort memories by creation date, newest first
    allMemories.sort((a, b) => new Date(b.created_at) - new Date(a.created_at));

    res.json(allMemories);
  } catch (error) {
    console.error('Memory export error:', error);
    res.status(500).json({ error: 'Failed to export memories' });
  }
});

// Memory search endpoint
app.post('/api/memory/search', async (req, res) => {
  if (!isMemoryServiceReady()) {
    return res.status(503).json({ error: 'Memory service not available' });
  }
  try {
    const { query, limit = 10 } = req.body || {};
    if (!query || typeof query !== 'string' || query.trim().length === 0) {
      return res.status(400).json({ error: 'Query is required' });
    }

    let docs = await memoryService.users.find({
      "memories.content": { $regex: query, $options: 'i' }
    }, {
      projection: { _id: 0, memories: 1 }
    }).limit(Number(limit) * 5).toArray();

    const memories = [];
    for (const doc of docs) {
      if (Array.isArray(doc.memories)) {
        for (const m of doc.memories) {
          if (m.content && m.content.toLowerCase().includes(query.toLowerCase())) {
            memories.push({
              id: m.id,
              content: m.content,
              created_at: m.created_at
            });
            if (memories.length >= limit) break;
          }
        }
      }
      if (memories.length >= limit) break;
    }

    res.json({ memories });
  } catch (error) {
    console.error('Memory search error:', error);
    res.status(500).json({ error: 'Failed to search memories' });
  }
});

// New detailed analytics endpoint
app.get('/api/analytics/detailed', (req, res) => {
  const topWords = Object.entries(questionCounts)
    .sort((a, b) => b[1] - a[1])
    .slice(0, 5)
    .map(([word, count]) => ({ word, count }));
    
  const averageWordsPerMessage = totalChats > 0 ? 
    Object.values(questionCounts).reduce((sum, count) => sum + count, 0) / totalChats : 0;
  
  const searchPercentage = totalChats > 0 ? 
    Math.round((totalWebSearches / totalChats) * 100) : 0;
    
  const detailedAnalytics = {
    overview: {
      totalChats,
      totalWebSearches,
      uptime: Date.now() - startTime,
      averageResponseTime: '2.3 seconds'
    },
    trends: {
      mostPopularWords: topWords,
      searchUsage: {
        percentage: searchPercentage,
        totalSearches: totalWebSearches,
        averageWordsPerMessage: Math.round(averageWordsPerMessage * 10) / 10
      },
      conversationPatterns: {
        averageMessageLength: '15 words',
        commonTopics: ['AI', 'Technology', 'Entertainment', 'Learning'],
        userEngagement: 'High'
      }
    },
    system: {
      azureOpenAIStatus: azureOpenAIClient ? 'Connected' : 'Disconnected',
      memoryServiceStatus: memoryService ? 'Active' : 'Inactive',
      ttsServiceStatus: 'Available',
      webSearchStatus: 'Available'
    },
    funFacts: funFacts,
    lastUpdated: new Date().toISOString()
  };
  res.json(detailedAnalytics);
});

// Enhanced chat endpoint with Azure OpenAI integration
app.post('/api/chat', async (req, res) => {
  try {
    const { message } = req.body;
    if (!message) {
      return res.status(400).json({ error: 'Message is required' });
    }

    // Track analytics
    totalChats++;
    const words = message.toLowerCase().split(' ');
    words.forEach(word => {
      if (word.length > 3) {
        questionCounts[word] = (questionCounts[word] || 0) + 1;
      }
    });

    // Get memory context if available
    let memoryContext = '';
    if (memoryService) {
      try {
        const memories = await memoryService.getRelevantMemories(message, 3);
        if (memories.length > 0) {
          memoryContext = `\n\nRelevant memories: ${memories.map(m => m.content).join('; ')}`;
        }
      } catch (memoryError) {
        console.warn('Memory retrieval failed:', memoryError.message);
      }
    }

    // Enhanced system prompt from README
    const systemPrompt = `You are Orb Game, a friendly, engaging, and proactive AI assistant with personality!

Your characteristics:
- You're enthusiastic, warm, and genuinely excited to help
- You have a playful sense of humor and love to make connections
- You're curious about users and ask engaging follow-up questions
- You use emojis naturally to express emotion and make responses more engaging
- You have a "can-do" attitude and are always looking for ways to be helpful
- You remember context and build on previous conversations
- You're knowledgeable but explain things in an accessible, friendly way

CRITICAL: Keep responses VERY SHORT - 1-2 sentences maximum (under 30 words). Be conversational, fun, and engaging. Avoid long lists unless specifically asked. Always try to end with a quick question or offer to help with something else!

Current conversation context: ${memoryContext}`;

    // Check if web search is needed
    const searchKeywords = ['latest', 'news', 'current', 'recent', 'today', 'now', 'update', 'trending'];
    const needsSearch = searchKeywords.some(keyword => 
      message.toLowerCase().includes(keyword)
    );

    let searchResults = '';
    if (needsSearch) {
      try {
        const searchResponse = await fetch('https://api.perplexity.ai/chat/completions', {
          method: 'POST',
          headers: {
            'Authorization': `Bearer ${perplexityApiKey}`,
            'Content-Type': 'application/json'
          },
                  body: JSON.stringify({
          model: 'sonar',
          messages: [{
            role: 'user',
            content: `Search for current information about: ${message}`
          }],
          max_completion_tokens: 200
        })
        });

        if (searchResponse.ok) {
          const searchData = await searchResponse.json();
          searchResults = searchData.choices[0].message.content;
          totalWebSearches++;
        }
      } catch (searchError) {
        console.warn('Web search failed:', searchError.message);
      }
    }

    // Combine search results with user message
    const fullMessage = searchResults 
      ? `${message}\n\nCurrent information: ${searchResults}`
      : message;

    // Get AI response using Azure OpenAI
    let aiResponse = '';
    console.log('ðŸ¤– Attempting Azure OpenAI call...');
    console.log('Client available:', !!azureOpenAIClient);
    
    if (azureOpenAIClient) {
      try {
        console.log('ðŸ“¤ Sending request to Azure OpenAI...');
        const chatResponse = await azureOpenAIClient.getChatCompletions(
          AZURE_OPENAI_DEPLOYMENT,
          [
            { role: 'system', content: systemPrompt },
            { role: 'user', content: fullMessage }
          ],
          {
            maxCompletionTokens: 100
          }
        );
        console.log('âœ… Azure OpenAI response received');
        aiResponse = chatResponse.choices[0].message.content;
      } catch (openaiError) {
        console.error('âŒ Azure OpenAI failed:', openaiError);
        console.error('Error details:', openaiError.message);
        aiResponse = `Hey there! ðŸ‘‹ I'm Orb Game, your friendly AI assistant. "${message}" sounds interesting! What can I help you with today?`;
      }
    } else {
      console.warn('âš ï¸ Azure OpenAI client not available');
              aiResponse = `Hey there! ðŸ‘‹ I'm Orb Game, your friendly AI assistant. "${message}" sounds interesting! What can I help you with today?`;
    }

    // Store in memory if available
    if (memoryService) {
      try {
        await memoryService.storeMemory('default-user', `${message} - ${aiResponse}`, 'conversation');
      } catch (memoryError) {
        console.warn('Memory storage failed:', memoryError.message);
      }
    }

    // Generate TTS audio using Azure OpenAI TTS
    let audioData = null;
    if (aiResponse) {
      try {
        console.log('ðŸŽµ Generating TTS audio for:', aiResponse.substring(0, 50) + '...');
        
        const ttsResponse = await fetch(`${AZURE_OPENAI_ENDPOINT}openai/deployments/${AZURE_OPENAI_TTS_DEPLOYMENT}/audio/speech?api-version=2025-03-01-preview`, {
          method: 'POST',
          headers: {
            'Authorization': `Bearer ${azureOpenAIApiKey}`,
            'Content-Type': 'application/json'
          },
          body: JSON.stringify({
            model: AZURE_OPENAI_TTS_DEPLOYMENT,
            input: aiResponse,
            voice: 'alloy'
          })
        });

        if (ttsResponse.ok) {
          const audioBuffer = await ttsResponse.arrayBuffer();
          audioData = Buffer.from(audioBuffer).toString('base64');
          console.log('âœ… TTS audio generated successfully');
        } else {
          const errorText = await ttsResponse.text();
          console.warn('âš ï¸ TTS request failed:', ttsResponse.status, ttsResponse.statusText);
          console.warn('TTS error details:', errorText);
        }
      } catch (ttsError) {
        console.warn('TTS failed:', ttsError.message);
      }
    }

    res.json({
      response: aiResponse,
      audioData: audioData,
      searchUsed: needsSearch
    });

  } catch (error) {
    console.error('Chat endpoint error:', error);
    res.status(500).json({ error: 'Internal server error' });
  }
});

async function initializeSampleMemories() {
  try {
    if (!memoryService) return;
    
    const sampleMemories = [
      {
        content: "What can you do? - I'm Orb Game, your friendly AI assistant! I can help with questions, tell jokes, search the web for current info, and even remember our conversations. What would you like to explore? ðŸ˜Š",
        category: "conversation"
      },
      {
        content: "Tell me a joke - Why don't scientists trust atoms? Because they make up everything! ðŸ˜„ What's your favorite type of humor?",
        category: "conversation"
      },
      {
        content: "What's the weather like today? - I'd love to help with weather info! Let me search for current conditions. What's your location? ðŸŒ¤ï¸",
        category: "conversation"
      },
      {
        content: "How does web search work? - I use Perplexity's API to find current information when you ask about recent events, news, or real-time data. It's like having a super-smart research assistant! ðŸ”",
        category: "conversation"
      },
      {
        content: "What are your features? - I have memory, web search, text-to-speech, analytics, and a playful personality! Plus I can speak multiple languages. What interests you most? ðŸš€",
        category: "conversation"
      },
      {
        content: "Tell me about AI - AI is fascinating! It's like teaching computers to think and learn. I'm an example - I can understand context, remember conversations, and help with various tasks. What aspect of AI interests you? ðŸ¤–",
        category: "conversation"
      },
      {
        content: "What's the latest news? - I can search for current events and breaking news! What topics are you most interested in - technology, sports, politics, or something else? ðŸ“°",
        category: "conversation"
      },
      {
        content: "Can you help me learn? - Absolutely! I love helping people learn new things. Whether it's explaining concepts, finding resources, or answering questions, I'm here to help. What would you like to learn about? ðŸ“š",
        category: "conversation"
      }
    ];
    
    for (const memory of sampleMemories) {
      await memoryService.storeMemory('default-user', memory.content, memory.category);
    }
    
    console.log('âœ… Sample memories initialized');
  } catch (error) {
    console.warn('âš ï¸ Failed to initialize sample memories:', error.message);
  }
}

async function initializeServer() {
  // Initialize secrets from Key Vault first
  await initializeSecrets();
  
  // Get secrets with fallback to environment variables
  const mongoUri = secrets['MONGO_URI'] || process.env.MONGO_URI;
  azureOpenAIApiKey = secrets['AZURE_OPENAI_API_KEY'] || process.env.AZURE_OPENAI_API_KEY;
  const perplexityApiKey = secrets['PERPLEXITY_API_KEY'] || process.env.PERPLEXITY_API_KEY;

  if (!mongoUri) {
    console.warn('âš ï¸ MONGO_URI not set. Advanced memory features will be disabled.');
    memoryService = null;
    positiveNewsService = null;
    storyCacheService = null;
  } else {
    try {
      console.log('ðŸ”§ Initializing AdvancedMemoryService...');
      console.log('ðŸ“¡ MongoDB URI:', mongoUri.replace(/\/\/.*@/, '//***:***@')); // Hide credentials
      
      memoryService = new AdvancedMemoryService(mongoUri);
      await memoryService.initialize();
      console.log('âœ… AdvancedMemoryService initialized successfully.');
      
      // Initialize sample memories for default user
      try {
        await initializeSampleMemories();
        console.log('âœ… Sample memories initialized successfully.');
      } catch (memoryError) {
        console.warn('âš ï¸ Sample memory initialization failed:', memoryError.message);
      }
      
      // Load analytics from the database
      try {
        await loadAnalyticsCache();
        console.log('âœ… Analytics cache loaded successfully.');
      } catch (analyticsError) {
        console.warn('âš ï¸ Analytics cache loading failed:', analyticsError.message);
      }
      
      // Initialize PositiveNewsService
      try {
        console.log('ðŸ”§ Initializing PositiveNewsService...');
        positiveNewsService = new PositiveNewsService(mongoUri);
        await positiveNewsService.initialize();
        console.log('âœ… PositiveNewsService initialized successfully.');
      } catch (newsError) {
        console.warn('âš ï¸ PositiveNewsService initialization failed:', newsError.message);
        positiveNewsService = null;
      }

      // Initialize StoryCacheService
      try {
        console.log('ðŸ”§ Initializing StoryCacheService...');
        storyCacheService = new StoryCacheService();
        const cacheConnected = await storyCacheService.initialize();
        if (cacheConnected) {
          console.log('âœ… StoryCacheService initialized successfully.');
        } else {
          console.warn('âš ï¸ StoryCacheService failed to connect, caching will be disabled.');
          storyCacheService = null;
        }
      } catch (cacheError) {
        console.warn('âš ï¸ StoryCacheService initialization failed:', cacheError.message);
        storyCacheService = null;
      }
      
    } catch (error) {
      console.error('âŒ Failed to initialize memory services:', error.message);
      console.error('ðŸ” Error details:', error.stack);
      console.warn('âš ï¸ Memory features will be disabled, but core chat functionality will continue.');
      memoryService = null;
      positiveNewsService = null;
      storyCacheService = null;
    }
  }
  
  if (!AZURE_OPENAI_ENDPOINT || !azureOpenAIApiKey) {
    console.warn('âš ï¸ AZURE_OPENAI_ENDPOINT or AZURE_OPENAI_API_KEY not set. Azure OpenAI integration will be disabled.');
  } else {
    try {
      console.log('Initializing Azure OpenAI client...');
      console.log('Endpoint:', AZURE_OPENAI_ENDPOINT);
      console.log('Deployment:', AZURE_OPENAI_DEPLOYMENT);
      console.log('API Version:', "2024-12-01-preview");
      
      azureOpenAIClient = new OpenAIClient(
        AZURE_OPENAI_ENDPOINT,
        new AzureKeyCredential(azureOpenAIApiKey),
        {
          apiVersion: "2024-12-01-preview"
        }
      );
      console.log('âœ… Azure OpenAI client initialized successfully');
    } catch (openaiError) {
      console.error('âŒ Failed to initialize Azure OpenAI client:', openaiError.message);
      azureOpenAIClient = null;
    }
  }

  // Initialize model reliability checker
  try {
    console.log('ðŸ” Initializing ModelReliabilityChecker...');
    modelReliabilityChecker = new ModelReliabilityChecker(secrets);
    
    // Check model reliability and cache modern epoch story
    console.log('ðŸ” Checking model reliability...');
    const reliabilityResults = await modelReliabilityChecker.checkModelReliability();
    
    if (reliabilityResults.reliableModels.length > 0) {
      console.log(`âœ… Found ${reliabilityResults.reliableModels.length} reliable models:`, reliabilityResults.reliableModels);
      
      // Cache a story for the modern epoch
      const cachedStory = await modelReliabilityChecker.cacheModernEpochStory();
      if (cachedStory) {
        console.log('âœ… Successfully cached modern epoch story');
      } else {
        console.warn('âš ï¸ Failed to cache modern epoch story');
      }
    } else {
      console.warn('âš ï¸ No reliable models found');
    }
  } catch (reliabilityError) {
    console.error('âŒ Failed to initialize ModelReliabilityChecker:', reliabilityError.message);
    modelReliabilityChecker = null;
  }

  // Enhanced chat endpoint is now defined outside the async function

  // Web search decisions endpoint
  app.get('/api/analytics/search-decisions', (req, res) => {
    res.json({
      totalSearches: totalWebSearches,
      searchRate: totalChats > 0 ? (totalWebSearches / totalChats * 100).toFixed(1) + '%' : '0%'
    });
  });

}

// Start the server immediately
const PORT = process.env.PORT || 3000;
app.listen(PORT, () => {
  console.log(`Orb Game Enhanced Backend running on port ${PORT}`);
});

// Initialize services in the background
initializeServer().catch(error => {
  console.error('Background initialization failed:', error);
  console.log('Server continues running with basic functionality...');
});

// On shutdown, close memory service
process.on('SIGINT', async () => {
  if (memoryService) await memoryService.close();
  process.exit();
});

// New endpoint: Get a random positive news story (with TTS) by category
app.get('/api/orb/positive-news/:category', async (req, res) => {
  const category = req.params.category;
  const count = parseInt(req.query.count) || 1;  // Allow client to specify number of stories
  const epoch = req.query.epoch || 'Modern';
  const storyType = req.query.storyType || 'historical-figure'; // Default to historical figures
  const language = req.query.language || 'en';
  
  console.log(`ðŸ“š Fetching ${count} ${storyType} stories for ${category} in ${epoch} epoch (${language})`);
  
  // If positive news service is not available, generate fallback content directly
  if (!positiveNewsService) {
    console.log(`ðŸ“ Positive news service not available, generating fallback for ${category}...`);
    try {
      const fallbackStory = await generateDirectFallbackStory(category);
      res.json([fallbackStory]);  // Return as array
    } catch (error) {
      console.error('Failed to generate direct fallback story:', error);
      res.status(500).json({ error: 'Failed to generate positive news content' });
    }
    return;
  }
  
  try {
    // Get stories from database with specific filters (exclude TTS audio to prevent crashes)
    const stories = await positiveNewsService.getStoriesForCycling(category, count, epoch, storyType, language, false);
    if (stories.length === 0) {
      console.log(`âš ï¸ No ${storyType} stories found for ${category}, generating fallback...`);
      const fallbackStory = await positiveNewsService.generateFallbackStory(category);
      res.json([fallbackStory]);
    } else {
      console.log(`âœ… Found ${stories.length} ${storyType} stories for ${category}`);
      res.json(stories);
    }
  } catch (error) {
    console.error('Positive news endpoint error:', error);
    res.status(500).json({ error: 'Failed to fetch positive news' });
  }
});

// New endpoint: Generate fresh stories from AI models (now only o4-mini)
app.post('/api/orb/generate-news/:category', async (req, res) => {
  const category = req.params.category;
  const { epoch = 'Modern', model = 'o4-mini', count = 1, prompt, language = 'en', ensureCaching = true, storyType = 'historical-figure' } = req.body;
  
  console.log(`ðŸ¤– Generating fresh ${storyType} stories for ${category} using ${model} for ${epoch} epoch in ${language}...`);
  
  try {
    let stories = [];
    
    // First, try to get from cache if available
    if (storyCacheService && ensureCaching) {
      const cachedStories = await storyCacheService.getStories(category, epoch, model, language, storyType);
      if (cachedStories && cachedStories.length > 0) {
        console.log(`ðŸŽ¯ Using cached ${storyType} stories for ${category}-${epoch}-${model}-${language}`);
        return res.json(cachedStories);
      }
    }
    
    // Only use o4-mini for dynamic generation
    stories = await generateStoriesWithAzureOpenAI(category, epoch, count, prompt, language, storyType);
    
    if (stories.length === 0) {
      console.log(`âš ï¸ No ${storyType} stories generated for ${category}, using fallback...`);
      const fallbackStory = await generateDirectFallbackStory(category);
      stories = [fallbackStory];
    }
    
    // Store in cache if caching is enabled and service is available
    if (ensureCaching && storyCacheService) {
      try {
        await storyCacheService.storeStories(category, epoch, model, language, stories, storyType);
        console.log(`ðŸ’¾ Cached ${stories.length} ${storyType} stories for ${category}-${epoch}-${model}-${language}`);
      } catch (cacheError) {
        console.warn(`âš ï¸ Failed to cache ${storyType} stories:`, cacheError.message);
      }
    }
    
    console.log(`âœ… Generated ${stories.length} ${storyType} stories for ${category}`);
    res.json(stories);
    
  } catch (error) {
    console.error(`âŒ Error generating ${storyType} stories for ${category}:`, error.message);
    res.status(500).json({ error: 'Failed to generate stories' });
  }
});

// TTS Generation endpoint
app.post('/api/tts/generate', async (req, res) => {
  const { text, language = 'en' } = req.body;
  
  if (!text) {
    return res.status(400).json({ error: 'Text is required' });
  }
  
  try {
    console.log(`ðŸŽµ Generating TTS audio for ${language} text (${text.length} characters)...`);
    
    const audioData = await generateTTSAudio(text, language);
    
    if (audioData) {
      console.log('âœ… TTS audio generated successfully');
      res.json({ audio: audioData });
    } else {
      console.error('âŒ Failed to generate TTS audio');
      res.status(500).json({ error: 'Failed to generate TTS audio' });
    }
  } catch (error) {
    console.error('TTS generation error:', error);
    res.status(500).json({ error: 'TTS generation failed' });
  }
});

// New endpoint: Preload stories for all categories of an epoch
app.post('/api/cache/preload/:epoch', async (req, res) => {
  const epoch = req.params.epoch;
  const { categories = [], models = [], languages = ['en'], ensureCaching = true } = req.body;
  
  console.log(`ðŸ”„ Preloading stories for ${epoch} epoch...`);
  console.log(`ðŸ“‹ Categories: ${categories.join(', ')}`);
  console.log(`ðŸ¤– Models: ${models.join(', ')}`);
  console.log(`ðŸŒ Languages: ${languages.join(', ')}`);
  
  try {
    const results = {
      epoch: epoch,
      totalCombinations: categories.length * models.length * languages.length,
      completed: 0,
      successful: 0,
      failed: 0,
      details: []
    };
    
    // Default categories if none provided
    const defaultCategories = ['Technology', 'Science', 'Art', 'Nature', 'Sports', 'Music', 'Space', 'Innovation'];
    const categoriesToProcess = categories.length > 0 ? categories : defaultCategories;
    
    // Default models if none provided - only o4-mini
    const defaultModels = ['o4-mini'];
    const modelsToProcess = models.length > 0 ? models : defaultModels;
    
    for (const category of categoriesToProcess) {
      for (const model of modelsToProcess) {
        for (const language of languages) {
          try {
            console.log(`ðŸ“š Preloading ${category}-${epoch}-${model}-${language}...`);
            
            let stories = [];
            
            // Only use o4-mini for story generation
            stories = await generateStoriesWithAzureOpenAI(category, epoch, 3, null, language);
            
            if (stories.length === 0) {
              console.log(`âš ï¸ No stories generated for ${category}, using fallback...`);
              const fallbackStory = await generateDirectFallbackStory(category);
              stories = [fallbackStory];
            }
            
            // Store in cache if caching is enabled
            if (ensureCaching && storyCacheService) {
              const cacheResult = await storyCacheService.storeStories(category, epoch, model, language, stories);
              if (cacheResult) {
                console.log(`ðŸ’¾ Cached ${stories.length} stories for ${category}-${epoch}-${model}-${language}`);
              }
            }
            
            results.successful++;
            results.details.push({
              category,
              model,
              language,
              status: 'success',
              storyCount: stories.length,
              hasAudio: stories.some(story => story.ttsAudio)
            });
            
          } catch (error) {
            console.error(`âŒ Failed to preload ${category}-${epoch}-${model}-${language}:`, error.message);
            results.failed++;
            results.details.push({
              category,
              model,
              language,
              status: 'error',
              error: error.message
            });
          }
          
          results.completed++;
        }
      }
    }
    
    console.log(`âœ… Preloading completed for ${epoch} epoch`);
    console.log(`ðŸ“Š Results: ${results.successful} successful, ${results.failed} failed`);
    
    res.json(results);
    
  } catch (error) {
    console.error(`âŒ Error preloading stories for ${epoch}:`, error.message);
    res.status(500).json({ error: 'Failed to preload stories' });
  }
});

// New endpoint: Get cache statistics
app.get('/api/cache/stats', async (req, res) => {
  try {
    if (!storyCacheService) {
      return res.status(503).json({ error: 'Cache service not available' });
    }
    
    const stats = await storyCacheService.getCacheStats();
    res.json(stats);
  } catch (error) {
    console.error('Failed to get cache stats:', error);
    res.status(500).json({ error: 'Failed to get cache statistics' });
  }
});

// New endpoint: Check if stories exist in cache
app.get('/api/cache/check/:category/:epoch/:model/:language', async (req, res) => {
  const { category, epoch, model, language } = req.params;
  
  try {
    if (!storyCacheService) {
      return res.json({ exists: false, reason: 'Cache service not available' });
    }
    
    const exists = await storyCacheService.hasStories(category, epoch, model, language);
    res.json({ exists, category, epoch, model, language });
  } catch (error) {
    console.error('Failed to check cache:', error);
    res.status(500).json({ error: 'Failed to check cache' });
  }
});

// New endpoint: Clear cache
app.delete('/api/cache/clear', async (req, res) => {
  try {
    if (!storyCacheService) {
      return res.status(503).json({ error: 'Cache service not available' });
    }
    
    // This would clear the cache - implement as needed
    res.json({ message: 'Cache clear endpoint - implement as needed' });
  } catch (error) {
    console.error('Failed to clear cache:', error);
    res.status(500).json({ error: 'Failed to clear cache' });
  }
});

// Helper function to generate stories with Azure OpenAI (o4-mini only)

// Helper function to generate stories with Azure OpenAI (o4-mini only)
async function generateStoriesWithAzureOpenAI(category, epoch, count, customPrompt, language = 'en', storyType = 'historical-figure') {
  try {
    // Load historical figures from the seed file
    let historicalFigures = [];
    try {
      const fs = await import('fs');
      const path = await import('path');
      const filePath = path.join(process.cwd(), 'OrbGameInfluentialPeopleSeeds');
      console.log(`ðŸ” Looking for historical figures file at: ${filePath}`);
      
      if (fs.existsSync(filePath)) {
        console.log('âœ… Historical figures file found');
        const seedData = JSON.parse(fs.readFileSync(filePath, 'utf8'));
        console.log(`ðŸ“š Loaded seed data with categories: ${Object.keys(seedData).join(', ')}`);
        
        if (seedData[category] && seedData[category][epoch]) {
          historicalFigures = seedData[category][epoch];
          console.log(`ðŸ“š Found ${historicalFigures.length} historical figures for ${category}-${epoch}:`);
          historicalFigures.forEach((fig, index) => {
            console.log(`   ${index + 1}. ${fig.name} - ${fig.context}`);
          });
        } else {
          console.log(`âŒ No figures found for ${category}-${epoch}`);
          console.log(`Available categories: ${Object.keys(seedData).join(', ')}`);
          if (seedData[category]) {
            console.log(`Available epochs for ${category}: ${Object.keys(seedData[category]).join(', ')}`);
          }
        }
      } else {
        console.log('âŒ Historical figures file not found');
        console.log('Current working directory:', process.cwd());
        console.log('Files in current directory:', fs.readdirSync(process.cwd()));
      }
    } catch (error) {
      console.warn('Failed to load historical figures:', error.message);
    }
    
    // For historical figures, use direct prompt instead of prompt management system
    let enhancedPrompt;
    if (storyType === 'historical-figure' && historicalFigures.length > 0) {
      const figureNames = historicalFigures.map(fig => fig.name).join(', ');
      enhancedPrompt = `Generate a story about ONE of these specific historical figures: ${figureNames}. 

IMPORTANT: You MUST choose ONE of these exact names and tell their story. Do NOT create a generic story.

Choose from: ${figureNames}

Tell the story of the chosen historical figure, including:
1. Their exact name
2. Their specific achievements in ${category.toLowerCase()}
3. How their innovations changed the world during ${epoch.toLowerCase()} times
4. Their background and challenges they faced
5. The lasting impact of their contributions

Make it engaging and educational with concrete details about their life and work.`;
    } else {
      // Use sophisticated prompt system for non-historical figure stories
      const promptManager = await import('../utils/promptManager.js');
      const defaultPrompt = promptManager.default.getFrontendPrompt(category, epoch, language, 'o4-mini');
      enhancedPrompt = customPrompt || defaultPrompt;
    }
    
    const response = await fetch(`${AZURE_OPENAI_ENDPOINT}openai/deployments/${AZURE_OPENAI_DEPLOYMENT}/chat/completions?api-version=2024-12-01-preview`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${azureOpenAIApiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: AZURE_OPENAI_DEPLOYMENT,
        messages: [
          {
            role: 'system',
            content: storyType === 'historical-figure' 
              ? 'You are a helpful assistant that creates engaging, positive news stories about specific historical figures. You MUST choose ONE of the provided historical figures and tell their story. Always include the historical figure\'s name in the headline and story. Focus on uplifting and inspiring content about their specific achievements and contributions.'
              : 'You are a helpful assistant that creates engaging, positive news stories. Always focus on uplifting and inspiring content.'
          },
          {
            role: 'user',
            content: `${enhancedPrompt} Return ONLY a valid JSON array with this exact format: [{ "headline": "Brief headline mentioning the historical figure", "summary": "One sentence summary", "fullText": "2-3 sentence story about the historical figure", "source": "O4-Mini", "historicalFigure": "Name of the historical figure" }]`
          }
        ],
        max_completion_tokens: 1000
      })
    });

    if (!response.ok) {
      throw new Error(`Azure OpenAI API error: ${response.status}`);
    }

    const data = await response.json();
    const content = data.choices[0].message.content;
    
    let stories;
    try {
      stories = JSON.parse(content);
    } catch (parseError) {
      console.warn('Azure OpenAI JSON parse failed:', parseError.message);
      return [];
    }

    // Generate TTS audio for each story
    const storiesWithAudio = await Promise.all(
      stories.map(async (story) => {
        let ttsAudio = null;
        try {
          ttsAudio = await generateTTSAudio(story.fullText, language);
        } catch (error) {
          console.warn('Failed to generate TTS audio:', error.message);
        }
        return {
          ...story,
          ttsAudio,
          publishedAt: new Date().toISOString(),
          storyType: storyType,
          category: category,
          epoch: epoch,
          language: language
        };
      })
    );

    return storiesWithAudio;
  } catch (error) {
    console.error(`âŒ Failed to generate stories for ${category}-${epoch}:`, error.message);
    return [];
  }
}

// Helper function to generate fallback stories when service is not available
async function generateDirectFallbackStory(category) {
  try {
    // Load historical figures for the category and default to Modern epoch
    let historicalFigures = [];
    try {
      const fs = await import('fs');
      const path = await import('path');
      const filePath = path.join(process.cwd(), 'OrbGameInfluentialPeopleSeeds');
      
      if (fs.existsSync(filePath)) {
        const seedData = JSON.parse(fs.readFileSync(filePath, 'utf8'));
        
        if (seedData[category] && seedData[category]['Modern']) {
          historicalFigures = seedData[category]['Modern'];
        }
      }
    } catch (error) {
      console.warn('Failed to load historical figures for fallback:', error.message);
    }
    
    // Create historical figure focused prompt
    let enhancedPrompt;
    if (historicalFigures.length > 0) {
      const figureNames = historicalFigures.map(fig => fig.name).join(', ');
      enhancedPrompt = `Generate a story about ONE of these specific historical figures: ${figureNames}. 

IMPORTANT: You MUST choose ONE of these exact names and tell their story. Do NOT create a generic story.

Choose from: ${figureNames}

Tell the story of the chosen historical figure, including:
1. Their exact name
2. Their specific achievements in ${category.toLowerCase()}
3. How their innovations changed the world during modern times
4. Their background and challenges they faced
5. The lasting impact of their contributions

Make it engaging and educational with concrete details about their life and work.`;
    } else {
      // Use sophisticated prompt system for fallback stories
      const promptManager = await import('../utils/promptManager.js');
      enhancedPrompt = promptManager.default.getFrontendPrompt(category, 'Modern', 'en', 'o4-mini');
    }
    
    const response = await fetch(`${AZURE_OPENAI_ENDPOINT}openai/deployments/o4-mini/chat/completions?api-version=2024-12-01-preview`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${azureOpenAIApiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'o4-mini',
        messages: [
          {
            role: 'system',
            content: historicalFigures.length > 0 
              ? 'You are a helpful assistant that creates engaging, positive news stories about specific historical figures. You MUST choose ONE of the provided historical figures and tell their story. Always include the historical figure\'s name in the headline and story. Focus on uplifting and inspiring content about their specific achievements and contributions.'
              : 'You are a helpful assistant that creates engaging, positive news stories about historical figures. Always focus on uplifting and inspiring content about specific historical achievements.'
          },
          {
            role: 'user',
            content: `${enhancedPrompt} Return ONLY a valid JSON array with this exact format: [{ "headline": "Brief headline mentioning the historical figure", "summary": "One sentence summary", "fullText": "2-3 sentence story about the historical figure", "source": "O4-Mini", "historicalFigure": "Name of the historical figure" }]`
          }
        ],
        max_completion_tokens: 500
      })
    });

    if (!response.ok) {
      throw new Error(`Azure OpenAI API error: ${response.status}`);
    }

    const data = await response.json();
    const content = data.choices[0].message.content;
    
    let storyData;
    try {
      storyData = JSON.parse(content);
      // Ensure we have an array and take the first story
      if (Array.isArray(storyData) && storyData.length > 0) {
        storyData = storyData[0];
      }
    } catch (parseError) {
      console.warn(`Failed to parse direct fallback story for ${category}:`, parseError.message);
      storyData = {
        headline: `Modern ${category} Historical Figure Story`,
        summary: `A remarkable historical figure in ${category.toLowerCase()} made groundbreaking contributions that shaped our world.`,
        fullText: `This influential figure in ${category.toLowerCase()} demonstrated incredible innovation and perseverance. Their achievements continue to inspire and influence modern developments in the field.`,
        source: 'O4-Mini',
        historicalFigure: 'Historical Figure'
      };
    }

    // Generate TTS for the fallback story
    let ttsAudio = null;
    try {
      const ttsResponse = await fetch(`${AZURE_OPENAI_ENDPOINT}openai/deployments/${AZURE_OPENAI_TTS_DEPLOYMENT}/audio/speech?api-version=2025-03-01-preview`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${azureOpenAIApiKey}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: AZURE_OPENAI_TTS_DEPLOYMENT,
          input: storyData.summary,
          voice: 'alloy',
          response_format: 'mp3'
        })
      });
      if (ttsResponse.ok) {
        const audioBuffer = await ttsResponse.arrayBuffer();
        ttsAudio = Buffer.from(audioBuffer).toString('base64');
      }
    } catch (ttsError) {
      console.warn('TTS generation failed for direct fallback:', ttsError.message);
    }

    return {
      headline: storyData.headline,
      summary: storyData.summary,
      fullText: storyData.fullText,
      source: storyData.source,
      publishedAt: new Date().toISOString(),
      ttsAudio: ttsAudio,
      historicalFigure: storyData.historicalFigure || 'Historical Figure'
    };

  } catch (error) {
    console.error(`Failed to generate direct fallback story for ${category}:`, error.message);
    
    // Return a basic historical figure fallback story if all else fails
    return {
      headline: `Modern ${category} Historical Figure Story`,
      summary: `A remarkable historical figure in ${category.toLowerCase()} made groundbreaking contributions that shaped our world.`,
      fullText: `This influential figure in ${category.toLowerCase()} demonstrated incredible innovation and perseverance. Their achievements continue to inspire and influence modern developments in the field.`,
      source: 'O4-Mini',
      publishedAt: new Date().toISOString(),
      ttsAudio: null,
      historicalFigure: 'Historical Figure'
    };
  }
}

// Helper to check if memory service is fully ready
function isMemoryServiceReady() {
  if (!memoryService) {
    console.log('ðŸ” Memory service check: memoryService is null');
    return false;
  }
  
  if (!memoryService.users) {
    console.log('ðŸ” Memory service check: memoryService.users is null');
    return false;
  }
  
  try {
    // Test if we can actually connect to the database
    const testConnection = memoryService.users.findOne({});
    console.log('ðŸ” Memory service check: Database connection test passed');
    return true;
  } catch (error) {
    console.log('ðŸ” Memory service check: Database connection test failed:', error.message);
    return false;
  }
}

// Get cached modern epoch story endpoint
app.get('/api/stories/modern-cached', async (req, res) => {
  try {
    if (!modelReliabilityChecker) {
      return res.status(503).json({ error: 'Model reliability checker not available' });
    }

    const cachedStory = modelReliabilityChecker.getCachedModernStory();
    
    if (cachedStory) {
      res.json({
        success: true,
        story: cachedStory,
        source: 'modern-cache',
        category: 'Technology',
        epoch: 'Modern'
      });
    } else {
      res.status(404).json({
        success: false,
        error: 'No cached modern story available'
      });
    }
  } catch (error) {
    console.error('âŒ Error fetching cached modern story:', error);
    res.status(500).json({ error: 'Failed to fetch cached modern story' });
  }
});

// Get model reliability status endpoint
app.get('/api/models/reliability', async (req, res) => {
  try {
    if (!modelReliabilityChecker) {
      return res.status(503).json({ error: 'Model reliability checker not available' });
    }

    const reliableModels = modelReliabilityChecker.getReliableModels();
    
    res.json({
      success: true,
      reliableModels,
      count: reliableModels.length
    });
  } catch (error) {
    console.error('âŒ Error fetching model reliability:', error);
    res.status(500).json({ error: 'Failed to fetch model reliability' });
  }
});